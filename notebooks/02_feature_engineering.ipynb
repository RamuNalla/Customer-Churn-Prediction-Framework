{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeleRetain: Phase 2 - Feature Engineering\n",
    "\n",
    "This notebook demonstrates the advanced feature engineering pipeline for customer churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\configs\\model_config.yaml\n",
      "..\\configs\\data_config.yaml\n",
      "..\\configs\\deployment_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "except NameError:\n",
    "    parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from src.feature_engineering import AdvancedFeatureEngineer\n",
    "from src.data.advanced_preprocessor import AdvancedPreprocessor, BusinessFeatureCreator\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "data_path = \"../data/raw/telco_customer_churn.csv\"\n",
    "df_raw = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Raw Dataset Shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_summary = df_raw.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"Missing Values:\")\n",
    "    for col, count in missing_summary.items():\n",
    "        percentage = (count / len(df_raw)) * 100\n",
    "        print(f\"  {col}: {count} ({percentage:.2f}%)\")\n",
    "else:\n",
    "    print(\"No missing values found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the advanced feature engineer\n",
    "feature_engineer = AdvancedFeatureEngineer(output_dir=\"../data/processed/\")\n",
    "\n",
    "print(\"Feature Engineering Configuration:\")\n",
    "print(f\"Target Column: {feature_engineer.config['target_column']}\")\n",
    "print(f\"Positive Class: {feature_engineer.config['positive_class']}\")\n",
    "print(f\"Business Features Enabled: {feature_engineer.config['feature_engineering']['business_features']}\")\n",
    "print(f\"Interaction Features Enabled: {feature_engineer.config['feature_engineering']['interactions']['create']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Business Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create business features\n",
    "business_creator = BusinessFeatureCreator(\n",
    "    create_clv=True,\n",
    "    create_service_metrics=True,\n",
    "    create_risk_scores=True,\n",
    "    create_customer_segments=True\n",
    ")\n",
    "\n",
    "df_with_business = business_creator.fit_transform(df_raw)\n",
    "print(f\"Shape after business features: {df_with_business.shape}\")\n",
    "print(f\"Business features created: {len(business_creator.feature_names_)}\")\n",
    "print(f\"New features: {business_creator.feature_names_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some key business features\n",
    "if 'CLV_estimate' in df_with_business.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # CLV distribution by churn\n",
    "    axes[0,0].hist([df_with_business[df_with_business['Churn']=='No']['CLV_estimate'].dropna(),\n",
    "                   df_with_business[df_with_business['Churn']=='Yes']['CLV_estimate'].dropna()],\n",
    "                  bins=30, alpha=0.7, label=['No Churn', 'Churn'])\n",
    "    axes[0,0].set_title('CLV Distribution by Churn Status')\n",
    "    axes[0,0].set_xlabel('Customer Lifetime Value')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Service adoption by churn\n",
    "    if 'total_services' in df_with_business.columns:\n",
    "        churn_by_services = df_with_business.groupby('total_services')['Churn'].apply(lambda x: (x=='Yes').mean() * 100)\n",
    "        axes[0,1].bar(churn_by_services.index, churn_by_services.values, alpha=0.7, color='orange')\n",
    "        axes[0,1].set_title('Churn Rate by Number of Services')\n",
    "        axes[0,1].set_xlabel('Total Services')\n",
    "        axes[0,1].set_ylabel('Churn Rate (%)')\n",
    "    \n",
    "    # Contract risk score distribution\n",
    "    if 'contract_risk_score' in df_with_business.columns:\n",
    "        axes[1,0].hist([df_with_business[df_with_business['Churn']=='No']['contract_risk_score'],\n",
    "                       df_with_business[df_with_business['Churn']=='Yes']['contract_risk_score']],\n",
    "                      bins=5, alpha=0.7, label=['No Churn', 'Churn'])\n",
    "        axes[1,0].set_title('Contract Risk Score Distribution')\n",
    "        axes[1,0].set_xlabel('Risk Score')\n",
    "        axes[1,0].legend()\n",
    "    \n",
    "    # Family score impact\n",
    "    if 'family_size_score' in df_with_business.columns:\n",
    "        churn_by_family = df_with_business.groupby('family_size_score')['Churn'].apply(lambda x: (x=='Yes').mean() * 100)\n",
    "        axes[1,1].bar(churn_by_family.index, churn_by_family.values, alpha=0.7, color='green')\n",
    "        axes[1,1].set_title('Churn Rate by Family Score')\n",
    "        axes[1,1].set_xlabel('Family Size Score')\n",
    "        axes[1,1].set_ylabel('Churn Rate (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "report_path = feature_engineer.run_complete_pipeline(\n",
    "    data_path=data_path,\n",
    "    target_col='Churn'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Feature engineering completed!\")\n",
    "print(f\"üìä Report generated: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data to examine results\n",
    "processed_data_path = \"../data/processed/train_processed.csv\"\n",
    "df_processed = pd.read_csv(processed_data_path)\n",
    "\n",
    "print(f\"Processed Dataset Shape: {df_processed.shape}\")\n",
    "print(f\"Original Dataset Shape: {df_raw.shape}\")\n",
    "print(f\"Features Added: {df_processed.shape[1] - df_raw.shape[1]}\")\n",
    "\n",
    "# Show new feature names\n",
    "original_features = set(df_raw.columns)\n",
    "new_features = [col for col in df_processed.columns if col not in original_features]\n",
    "print(f\"\\nNew Features Created ({len(new_features)}):\")\n",
    "for i, feature in enumerate(new_features[:20], 1):  # Show first 20\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "if len(new_features) > 20:\n",
    "    print(f\"  ... and {len(new_features) - 20} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distributions\n",
    "numeric_features = df_processed.select_dtypes(include=[np.number]).columns\n",
    "categorical_features = df_processed.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"Feature Distribution Analysis:\")\n",
    "print(f\"  Total Features: {len(df_processed.columns)}\")\n",
    "print(f\"  Numeric Features: {len(numeric_features)}\")\n",
    "print(f\"  Categorical Features: {len(categorical_features)}\")\n",
    "\n",
    "# Check for binary features\n",
    "binary_features = [col for col in numeric_features \n",
    "                  if df_processed[col].nunique() == 2 and \n",
    "                     set(df_processed[col].unique()).issubset({0, 1})]\n",
    "print(f\"  Binary Features: {len(binary_features)}\")\n",
    "\n",
    "# Check data quality\n",
    "missing_after = df_processed.isnull().sum().sum()\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"  Missing values: {missing_after}\")\n",
    "print(f\"  Duplicates: {df_processed.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "if len(numeric_features) > 1:\n",
    "    # Calculate correlations with target (if numeric)\n",
    "    target_col = 'Churn'\n",
    "    if target_col in df_processed.columns:\n",
    "        # Convert target to numeric for correlation\n",
    "        target_numeric = (df_processed[target_col] == 'Yes').astype(int) if df_processed[target_col].dtype == 'object' else df_processed[target_col]\n",
    "        \n",
    "        # Get correlations with target\n",
    "        correlations = df_processed[numeric_features].corrwith(target_numeric).abs().sort_values(ascending=False)\n",
    "        correlations = correlations.dropna()\n",
    "        \n",
    "        print(f\"\\nTop 15 Features by Correlation with {target_col}:\")\n",
    "        for i, (feature, corr) in enumerate(correlations.head(15).items(), 1):\n",
    "            print(f\"  {i:2d}. {feature}: {corr:.4f}\")\n",
    "        \n",
    "        # Plot top correlations\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_correlations = correlations.head(15)\n",
    "        plt.barh(range(len(top_correlations)), top_correlations.values)\n",
    "        plt.yticks(range(len(top_correlations)), top_correlations.index)\n",
    "        plt.xlabel('Absolute Correlation with Churn')\n",
    "        plt.title('Top 15 Features by Correlation with Churn')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if 'Churn' in df_processed.columns:\n",
    "    # Prepare data for feature importance\n",
    "    X = df_processed.drop(['Churn'], axis=1)\n",
    "    y = (df_processed['Churn'] == 'Yes').astype(int)\n",
    "    \n",
    "    # Quick train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train Random Forest for feature importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 15 Features by Random Forest Importance:\")\n",
    "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "        print(f\"  {i:2d}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Features by Random Forest Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Model performance on processed features\n",
    "    from sklearn.metrics import classification_report, roc_auc_score\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\nüéØ Quick Model Performance on Engineered Features:\")\n",
    "    print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance before and after feature engineering\n",
    "print(\"üìä Comparing Model Performance: Before vs After Feature Engineering\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare original data (basic preprocessing only)\n",
    "df_basic = df_raw.copy()\n",
    "\n",
    "# Basic preprocessing for original data\n",
    "# Handle missing values in TotalCharges\n",
    "df_basic['TotalCharges'] = pd.to_numeric(df_basic['TotalCharges'], errors='coerce')\n",
    "df_basic['TotalCharges'].fillna(df_basic['MonthlyCharges'], inplace=True)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "categorical_columns = df_basic.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'Churn' in categorical_columns:\n",
    "    categorical_columns.remove('Churn')\n",
    "    \n",
    "df_basic_encoded = pd.get_dummies(df_basic, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Prepare datasets\n",
    "X_basic = df_basic_encoded.drop(['Churn'], axis=1)\n",
    "X_engineered = df_processed.drop(['Churn'], axis=1)\n",
    "y_basic = (df_basic['Churn'] == 'Yes').astype(int)\n",
    "y_engineered = (df_processed['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "# Train-test splits\n",
    "X_basic_train, X_basic_test, y_basic_train, y_basic_test = train_test_split(\n",
    "    X_basic, y_basic, test_size=0.3, random_state=42, stratify=y_basic)\n",
    "\n",
    "X_eng_train, X_eng_test, y_eng_train, y_eng_test = train_test_split(\n",
    "    X_engineered, y_engineered, test_size=0.3, random_state=42, stratify=y_engineered)\n",
    "\n",
    "# Train models\n",
    "rf_basic = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_engineered = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "rf_basic.fit(X_basic_train, y_basic_train)\n",
    "rf_engineered.fit(X_eng_train, y_eng_train)\n",
    "\n",
    "# Predictions\n",
    "y_basic_pred_proba = rf_basic.predict_proba(X_basic_test)[:, 1]\n",
    "y_eng_pred_proba = rf_engineered.predict_proba(X_eng_test)[:, 1]\n",
    "\n",
    "# Performance comparison\n",
    "basic_auc = roc_auc_score(y_basic_test, y_basic_pred_proba)\n",
    "engineered_auc = roc_auc_score(y_eng_test, y_eng_pred_proba)\n",
    "improvement = engineered_auc - basic_auc\n",
    "\n",
    "print(f\"Basic Features (Original):     AUC = {basic_auc:.4f} | Features: {X_basic.shape[1]}\")\n",
    "print(f\"Engineered Features:           AUC = {engineered_auc:.4f} | Features: {X_engineered.shape[1]}\")\n",
    "print(f\"Improvement:                   +{improvement:.4f} ({improvement/basic_auc*100:.1f}% relative)\")\n",
    "print(f\"\\nüéâ Feature engineering {'improved' if improvement > 0 else 'did not improve'} model performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ PHASE 2 FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Original Features: {df_raw.shape[1]}\")\n",
    "print(f\"‚úÖ Final Features: {df_processed.shape[1]}\")\n",
    "print(f\"‚úÖ Features Added: {df_processed.shape[1] - df_raw.shape[1]}\")\n",
    "print(f\"‚úÖ Data Quality: {df_processed.isnull().sum().sum()} missing values (vs {df_raw.isnull().sum().sum()} originally)\")\n",
    "print(f\"‚úÖ Model Performance: AUC improved by {improvement:.4f}\")\n",
    "print(f\"\\nüìÅ Outputs Generated:\")\n",
    "print(f\"  ‚Ä¢ Processed Data: ../data/processed/train_processed.csv\")\n",
    "print(f\"  ‚Ä¢ Feature Report: ../data/processed/feature_engineering_report.json\")\n",
    "print(f\"  ‚Ä¢ Preprocessing Pipeline: ../data/processed/preprocessing_artifacts.pkl\")\n",
    "print(f\"  ‚Ä¢ HTML Report: {report_path}\")\n",
    "print(f\"\\nüöÄ Ready for Phase 3: Model Development & Training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
